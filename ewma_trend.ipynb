{
 "cells": [
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.tsa.stattools import coint, grangercausalitytests, adfuller\n",
    "from scipy.stats import pearsonr\n",
    "import pytz\n",
    "import requests\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "API_KEY_ID     = \"PKL9BT6QPZNC5690ELR5\"\n",
    "API_SECRET_KEY = \"8N12VHY2aZ4qbMHar3gvWdDJpaws5fRZMyQZO3Bf\"\n",
    "\n",
    "\n",
    "# Step 1: Data Retrieval using Alpaca API\n",
    "def fetch_alpaca_data(symbol, timeframe, start_date, end_date='2025-03-17'):\n",
    "    url = 'https://data.alpaca.markets/v2/stocks/bars'\n",
    "    headers = {\n",
    "        'APCA-API-KEY-ID': API_KEY_ID,\n",
    "        'APCA-API-SECRET-KEY': API_SECRET_KEY\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'symbols': symbol,\n",
    "        'timeframe': timeframe,\n",
    "        'start': dt.datetime.strptime(start_date, \"%Y-%m-%d\").isoformat() + 'Z',\n",
    "        'end': dt.datetime.strptime(end_date, \"%Y-%m-%d\").isoformat() + 'Z',\n",
    "        'limit': 10000,\n",
    "        'adjustment': 'raw',\n",
    "        'feed': 'sip'\n",
    "    }\n",
    "\n",
    "    data_list = []\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    utc = pytz.utc\n",
    "\n",
    "    print(\"Starting data fetch...\")\n",
    "    while True:\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Error fetching data with status code {response.status_code}: {response.text}\")\n",
    "            break\n",
    "\n",
    "        data = response.json()\n",
    "        bars = data.get('bars')\n",
    "\n",
    "        for symbol, entries in bars.items():\n",
    "            for entry in entries:\n",
    "                try:\n",
    "                    utc_time = dt.datetime.fromisoformat(entry['t'].rstrip('Z')).replace(tzinfo=utc)\n",
    "                    eastern_time = utc_time.astimezone(eastern)\n",
    "                    data_entry = {\n",
    "                        'DATE': eastern_time,\n",
    "                        f'{symbol}': entry['c']\n",
    "                    }\n",
    "                    data_list.append(data_entry)\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        if 'next_page_token' in data and data['next_page_token']:\n",
    "            params['page_token'] = data['next_page_token']\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(data_list)\n",
    "    print(\"Data fetching complete.\")\n",
    "    return df\n",
    "\n",
    "# Step 2: Lead-Lag Analysis using Cointegration\n",
    "def cointegration_test(series1, series2):\n",
    "    coint_score, p_value, _ = coint(series1, series2)\n",
    "    return coint_score, p_value\n",
    "\n",
    "# Step 3: Lead-Lag Analysis using EWMA\n",
    "def ewma_lead_lag(series1, series2, span=60):\n",
    "    ewma1 = series1.ewm(span=span).mean()\n",
    "    ewma2 = series2.ewm(span=span).mean()\n",
    "    correlation, _ = pearsonr(ewma1.dropna(), ewma2.dropna())\n",
    "    return ewma1, ewma2, correlation\n",
    "\n",
    "# Step 4: Granger Causality for Optimal Lag\n",
    "def granger_causality(series1, series2, maxlag=30):\n",
    "    data = pd.concat([series1, series2], axis=1).dropna()\n",
    "    result = grangercausalitytests(data, maxlag=maxlag, verbose=False)\n",
    "    p_values = [round(result[i+1][0]['ssr_ftest'][1], 4) for i in range(maxlag)]\n",
    "    optimal_lag = p_values.index(min(p_values)) + 1\n",
    "    return optimal_lag, p_values\n",
    "\n",
    "# Main Execution\n",
    "stock1_data = fetch_alpaca_data('NVDA', '1Min', '2025-01-01')\n",
    "stock2_data = fetch_alpaca_data('TSLA', '1Min', '2025-01-01')\n",
    "\n",
    "# Synchronize the data on dates\n",
    "merged_data = pd.merge(stock1_data, stock2_data, on='DATE', how='inner')\n",
    "\n",
    "# Calculate Cointegration\n",
    "score, p_value = cointegration_test(merged_data['NVDA'], merged_data['TSLA'])\n",
    "print(f'Cointegration Test: Score={score:.4f}, P-Value={p_value:.4f}')\n",
    "\n",
    "# Calculate EWMA\n",
    "ewma1, ewma2, correlation = ewma_lead_lag(merged_data['NVDA'], merged_data['TSLA'])\n",
    "print(f'EWMA Correlation: {correlation:.4f}')\n",
    "\n",
    "# Optimal Lag via Granger Causality\n",
    "lag, p_values = granger_causality(merged_data['NVDA'], merged_data['TSLA'])\n",
    "print(f'Optimal Lag: {lag} | P-Values: {p_values}')\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Intraday Prices')\n",
    "plt.plot(merged_data['DATE'], merged_data['NVDA'], label='NVDA')\n",
    "plt.plot(merged_data['DATE'], merged_data['TSLA'], label='TSLA', alpha=0.7)\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('EWMA of Returns')\n",
    "plt.plot(ewma1, label='NVDA EWMA')\n",
    "plt.plot(ewma2, label='TSLA EWMA', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "114a3e367b2c98f6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Simulating merged_data for demonstration purposes\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range(start=\"2025-01-01\", periods=100, freq=\"T\")\n",
    "spy_prices = np.cumsum(np.random.randn(100)) + 100\n",
    "qqq_prices = np.cumsum(np.random.randn(100)) + 200\n",
    "merged_data = pd.DataFrame({\"DATE\": dates, \"SPY\": spy_prices, \"QQQ\": qqq_prices})\n",
    "\n",
    "# Define a function to calculate EWMA with a given lag\n",
    "def calculate_ewma(data, lag):\n",
    "    alpha = 2 / (lag + 1)  # Smoothing factor based on lag\n",
    "    return data.ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "# Compute EWMA for a range of lags and store results\n",
    "lags = range(1, 11)\n",
    "ewma_results = {}\n",
    "for lag in lags:\n",
    "    ewma_results[lag] = {\n",
    "        'EWMA_SPY': calculate_ewma(merged_data['SPY'], lag),\n",
    "        'EWMA_QQQ': calculate_ewma(merged_data['QQQ'], lag)\n",
    "    }\n",
    "\n",
    "# Add EWMA with an example lag (e.g., lag=5) to the dataframe\n",
    "example_lag = 5\n",
    "merged_data['EWMA_SPY'] = ewma_results[example_lag]['EWMA_SPY']\n",
    "merged_data['EWMA_QQQ'] = ewma_results[example_lag]['EWMA_QQQ']\n",
    "\n",
    "# Perform linear regression on SPY and QQQ prices\n",
    "X = merged_data['SPY'].values.reshape(-1, 1)  # Independent variable\n",
    "Y = merged_data['QQQ'].values  # Dependent variable\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Get regression coefficients\n",
    "slope = model.coef_[0]\n",
    "intercept = model.intercept_\n",
    "\n",
    "# Predict QQQ prices using the regression model\n",
    "merged_data['QQQ_Predicted'] = model.predict(X)\n",
    "\n",
    "# Create lagged features for SPY and QQQ prices\n",
    "max_lag = 10\n",
    "for lag in range(1, max_lag + 1):\n",
    "    merged_data[f'SPY_Lag_{lag}'] = merged_data['SPY'].shift(lag)\n",
    "    merged_data[f'QQQ_Lag_{lag}'] = merged_data['QQQ'].shift(lag)\n",
    "\n",
    "# Drop rows with NaN values due to lagging\n",
    "merged_data.dropna(inplace=True)\n",
    "\n",
    "# Train a Random Forest model using lagged features\n",
    "features = [col for col in merged_data.columns if 'Lag' in col]\n",
    "X_rf = merged_data[features]\n",
    "Y_rf = merged_data['QQQ']\n",
    "\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_rf, Y_rf)\n",
    "\n",
    "# Plotting EWMA for all lags\n",
    "plt.figure(figsize=(12, 6))\n",
    "for lag in lags:\n",
    "    plt.plot(ewma_results[lag]['EWMA_SPY'], label=f'SPY EWMA Lag {lag}')\n",
    "plt.legend()\n",
    "plt.title('EWMA for SPY with Different Lags')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "for lag in lags:\n",
    "    plt.plot(ewma_results[lag]['EWMA_QQQ'], label=f'QQQ EWMA Lag {lag}')\n",
    "plt.legend()\n",
    "plt.title('EWMA for QQQ with Different Lags')\n",
    "plt.show()\n",
    "\n",
    "# Output results: Regression coefficients and Random Forest feature importance\n",
    "print(f\"Linear Regression Slope: {slope}\")\n",
    "print(f\"Linear Regression Intercept: {intercept}\")\n",
    "print(\"Random Forest Feature Importance:\")\n",
    "print(rf_model.feature_importances_)\n"
   ],
   "id": "f2adbb435e55e359",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# Fetch data for SPY and QQQ\n",
    "spy_data = fetch_alpaca_data('SPY', '1Min', '2025-01-01')\n",
    "qqq_data = fetch_alpaca_data('QQQ', '1Min', '2025-01-01')\n",
    "\n",
    "# Merge data on date\n",
    "merged_data = pd.merge(spy_data, qqq_data, on='DATE', how='inner')\n",
    "\n",
    "# Perform cointegration test\n",
    "score, pvalue, _ = coint(merged_data['SPY'], merged_data['QQQ'])\n",
    "print(f\"Cointegration Test: p-value = {pvalue}\")\n",
    "\n",
    "# If p-value < 0.05, the series are likely cointegrated\n",
    "if pvalue < 0.05:\n",
    "    print(\"Series are cointegrated.\")\n",
    "else:\n",
    "    print(\"Series are not cointegrated.\")\n",
    "\n",
    "# Calculate EWMA\n",
    "alpha = 0.1  # Smoothing factor\n",
    "ewma_spy = merged_data['SPY'].ewm(alpha=alpha, adjust=False).mean()\n",
    "ewma_qqq = merged_data['QQQ'].ewm(alpha=alpha, adjust=False).mean()\n",
    "\n",
    "# Plot EWMA\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(ewma_spy, label='SPY EWMA')\n",
    "plt.plot(ewma_qqq, label='QQQ EWMA')\n",
    "plt.legend()\n",
    "plt.title('EWMA Comparison')\n",
    "plt.show()\n",
    "\n",
    "# Perform Granger causality test to check for lead-lag relationship\n",
    "test_result = grangercausalitytests(merged_data[['SPY', 'QQQ']], maxlag=10, verbose=False)\n",
    "for lag, result in test_result.items():\n",
    "    print(f\"Lag: {lag}, p-value: {result[0]['ssr_chi2test'][1]}\")\n",
    "\n",
    "# Prepare data for regression\n",
    "X = merged_data[['SPY']]\n",
    "y = merged_data['QQQ']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Linear Regression\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Determine optimal lag using Granger causality results\n",
    "optimal_lag = min([lag for lag, result in test_result.items() if result[0]['ssr_chi2test'][1] < 0.05], default=None)\n",
    "if optimal_lag:\n",
    "    print(f\"Optimal lag based on Granger causality: {optimal_lag}\")\n",
    "else:\n",
    "    print(\"No significant lag found.\")\n",
    "\n",
    "# Plot original data with lag\n",
    "if optimal_lag:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(merged_data['SPY'], label='SPY')\n",
    "    plt.plot(merged_data['QQQ'].shift(optimal_lag), label=f'QQQ shifted by {optimal_lag}')\n",
    "    plt.legend()\n",
    "    plt.title('Lead-Lag Relationship')\n",
    "    plt.show()\n"
   ],
   "id": "e26e018f53ab9c66",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
